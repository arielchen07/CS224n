model:
  path: "meta-llama/Llama-3.2-3B-Instruct"
  kwargs:
    use_cache: false
    trust_remote_code: true
    attn_implementation: "eager"
    torch_dtype: "bfloat16"
  tokenizer:
    pad_token: "<|finetune_right_pad_id|>"
    padding_side: "right"
    max_length: 2048

training:
  # hyperparameters to tune
  learning_rate: 5.0e-06
  num_train_epochs: 2
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 1
  warmup_ratio: 0.2
  weight_decay: 0.01
  lr_scheduler_type: "cosine"
  
  # Output and checkpointing
  output_dir: "./checkpoints/llama_3b"
  save_steps: 100
  save_total_limit: 1
  logging_steps: 20

peft:
  # LoRA hyperparameters
  r: 4
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules: "all-linear" 

inference_checkpoint: 
  path: "./checkpoints/llama_3b/checkpoint-4"

inference:
  max_new_tokens: 500
  return_full_text: false
  temperature: 0.0
  do_sample: false
  # top_p: 0.9
  # top_k: 0
