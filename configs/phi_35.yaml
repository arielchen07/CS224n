model:
  path: "microsoft/Phi-3.5-mini-instruct"
  kwargs:
    use_cache: false
    trust_remote_code: true
    attn_implementation: "eager"
    torch_dtype: "bfloat16"
  tokenizer:
    pad_token: "<|endoftext|>"
    padding_side: "right"
    max_length: 2048

training:
  # hyperparameters to tune
  learning_rate: 5.0e-06
  num_train_epochs: 2
  logging_steps: 20
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 1
  warmup_ratio: 0.2
  weight_decay: 0.01
  lr_scheduler_type: "cosine"
  
  # Output and checkpointing
  output_dir:  "./checkpoints/phi"
  save_steps: 500 
  save_total_limit: 20
  # remove later
  do_eval: True  
  evaluation_strategy: "steps"  
  eval_steps: 500

peft:
  # LoRA hyperparameters
  r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules: "all-linear" 
